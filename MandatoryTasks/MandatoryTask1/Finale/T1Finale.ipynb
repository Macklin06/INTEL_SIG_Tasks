{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "048c8f1e"
      },
      "source": [
        "## Task 1: Building a Question Answering and Translation System\n",
        "\n",
        "Hey everyone! For this project, I built a system that can answer questions and then translate those answers into Hindi. I used some great libraries from the `transformers` library by Hugging Face.\n",
        "\n",
        "Let's break down how we're doing this, step by step!\n",
        "\n",
        "First, we need to import the necessary tools from the `transformers` library. We'll need `pipeline` for easily using pre-trained models, `AutoModelForQuestionAnswering` and `AutoTokenizer` for our question answering part, and `AutoModelForSeq2SeqLM` for our translation part."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bd54259e"
      },
      "source": [
        "# Import required libraries\n",
        "from transformers import pipeline\n",
        "from transformers import AutoModelForQuestionAnswering, AutoTokenizer\n",
        "from transformers import AutoModelForSeq2SeqLM"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5a7d68fb"
      },
      "source": [
        "Next, we're setting up our Question Answering (QA) model. We're using a pre-trained model called \"deepset/roberta-base-squad2\" which is good at answering questions based on a given text as it is trained using SQuAD 2.0 dataset which includes unanswerable questions,and it also achieved high Exact Match and F1 scores in that dataset.\n",
        "\n",
        "The `pipeline()` function is helpful here because it takes care of loading both the model and its tokenizer (which helps the model understand the text) in one go. This makes things much simpler!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6841b4b6",
        "outputId": "f13b0d52-5385-4dff-a9d8-0293495ebe57"
      },
      "source": [
        "# Load the Question Answering (QA) model\n",
        "\n",
        "# We specify the task 'question-answering'\n",
        "# and the model name we chose.\n",
        "qa_model_name = \"deepset/roberta-base-squad2\"\n",
        "\n",
        "# The pipeline() function handles loading the model and tokenizer for us.\n",
        "# This is much easier than loading them separately.\n",
        "qa_pipeline = pipeline('question-answering', model=qa_model_name, tokenizer=qa_model_name)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "39a16b32"
      },
      "source": [
        "Now, we need to load our Translation model. We want to translate from English to Hindi, so we're using a model specifically trained for this from the Helsinki-NLP group called \"Helsinki-NLP/opus-mt-en-hi\" which I also used in PartB of this Task.\n",
        "\n",
        "Unlike the QA model where we used `pipeline`, here we're loading the tokenizer and the model separately using `AutoTokenizer.from_pretrained()` and `AutoModelForSeq2SeqLM.from_pretrained()`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a3f8edff",
        "outputId": "c49dd8e8-3930-4248-cd0a-c3d57d3c5aab"
      },
      "source": [
        "# Load the Translation (English to Hindi) model\n",
        "\n",
        "# We need an English-to-Hindi model.\n",
        "# The Helsinki-NLP opus-mt models are great for this.\n",
        "trans_model_name = \"Helsinki-NLP/opus-mt-en-hi\"\n",
        "\n",
        "# Load the tokenizer and model separately, like we did in Part B\n",
        "trans_tokenizer = AutoTokenizer.from_pretrained(trans_model_name)\n",
        "trans_model = AutoModelForSeq2SeqLM.from_pretrained(trans_model_name)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/transformers/models/marian/tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
            "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0c927f5c"
      },
      "source": [
        "This is where we put it all together! We've created a Python function called `get_hindi_answer`\n",
        "\n",
        "This function takes an English question and some text (the context) as input.\n",
        "\n",
        "Inside the function:\n",
        "1. It first uses our QA pipeline (`qa_pipeline`) to find the answer to the English question within the given context.\n",
        "2. Then, it takes the English answer it found and uses the translation model (`trans_model`) and its tokenizer (`trans_tokenizer`) to translate it into Hindi.\n",
        "3. Finally, it prints out the original question, context, the English answer, and the translated Hindi answer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "023235cd"
      },
      "source": [
        "# Create a function to combine both the stages\n",
        "\n",
        "def get_hindi_answer(question, context):\n",
        "    \"\"\"\n",
        "    Takes an English question and context, finds the English answer,\n",
        "    and translates it to Hindi.\n",
        "    \"\"\"\n",
        "\n",
        "    print(f\"Original Question: {question}\")\n",
        "    print(f\"Original Context: {context[:50]}...\") # Prints first 50 chars only\n",
        "\n",
        "    #Step 1: Get english answer\n",
        "    qa_result = qa_pipeline(question=question, context=context)\n",
        "    english_answer = qa_result['answer']\n",
        "\n",
        "    print(f\"Found English Answer: {english_answer}\")\n",
        "\n",
        "    # Step 2: Translate to hindi\n",
        "\n",
        "    # Tokenize the english answer\n",
        "    # We put it in a list [english_answer] because the translation model\n",
        "    # expects a batch of sentences.\n",
        "    inputs = trans_tokenizer([english_answer], return_tensors=\"pt\", padding=True, truncation=True)\n",
        "\n",
        "    # Generate translation\n",
        "    outputs = trans_model.generate(**inputs)\n",
        "\n",
        "    # Decode translation, and output[0] because only one entry in the list\n",
        "    hindi_answer = trans_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    print(f\"Hindi Answer: {hindi_answer}\")\n",
        "    return hindi_answer"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a75b7a8d"
      },
      "source": [
        "To Test or see the system working, we're running a little demonstration.\n",
        "\n",
        "We define a sample English question and context. Then, we call our `get_hindi_answer` function with these inputs.\n",
        "\n",
        "The output shows the steps our function takes and the final Hindi answer it produces. It looks like it's working!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0107fe09",
        "outputId": "56bf5abc-c877-4069-f601-d3bcb3b40160"
      },
      "source": [
        "# Run a demonstration\n",
        "\n",
        "# define our example question and context\n",
        "demo_question = \"What was Leo's secret fear?\"\n",
        "demo_context = 'Leo the lion was known throughout the savanna for his majestic roar, which could be heard for miles. However, Leo had a secret: he was afraid of the dark. Every night, as the sun dipped below the horizon, he would quietly retreat to his cave and curl up with his favorite, worn-out toy, a stuffed giraffe named Gary. The other animals never knew of his fear, and Leo hoped they never would. His reputation as the bravest animal depended on it.'\n",
        "\n",
        "print(\"---- Running System Demonstration -----\")\n",
        "print(\"\") # Add a blank line for spacing\n",
        "\n",
        "#Call our function with the demo inputs\n",
        "final_answer = get_hindi_answer(question=demo_question, context=demo_context)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---- Running System Demonstration -----\n",
            "\n",
            "Original Question: What was Leo's secret fear?\n",
            "Original Context: Leo the lion was known throughout the savanna for ...\n",
            "Found English Answer: the dark\n",
            "Hindi Answer: अंधेरे\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's Try with another similar example\n"
      ],
      "metadata": {
        "id": "gPKU_yNOSvvJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run a demonstration2\n",
        "\n",
        "# define our example question and context\n",
        "demo_question = \"What was Elias's secret and what was the true source of his bravery?\"\n",
        "demo_context = 'The old lighthouse keeper, Elias, was known for his stoic demeanor and unwavering commitment to his post. For forty years, he had kept the light burning brightly through the most ferocious storms, never once faltering in his duty. The fishermen swore that the lighthouse was protected by Elias\\'s own iron will. But in truth, Elias\\'s hands shook with a terrible fear every time a storm approached. The thunder and lightning reminded him of the shipwreck that had claimed his family long ago, and he feared that one day, he would fail and another vessel would meet the same fate. His bravery was not the absence of fear, but a constant, silent battle against it.'\n",
        "\n",
        "print(\"---- Running System Demonstration -----\")\n",
        "print(\"\") # Add a blank line for spacing\n",
        "\n",
        "#Call our function with the demo inputs\n",
        "final_answer = get_hindi_answer(question=demo_question, context=demo_context)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A-NRU_zUSzMC",
        "outputId": "43f1cdb0-a359-4035-831a-11cff3bb4520"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---- Running System Demonstration -----\n",
            "\n",
            "Original Question: What was Elias's secret and what was the true source of his bravery?\n",
            "Original Context: The old lighthouse keeper, Elias, was known for hi...\n",
            "Found English Answer: fear\n",
            "Hindi Answer: डर\n"
          ]
        }
      ]
    }
  ]
}